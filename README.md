# big_data_processing
Pressing a large dataset Approx: 20GB

Here I used spark to read and write large dataset that is stored in data folder.
My Class to handle All task relative to the dataset is in the spark_handler.py file called SparkCSVConcater.


To Run:

    1. First Run:  !pipenv install
    2. Run : !pipenv shell
    3. Run: !python3 main.py
